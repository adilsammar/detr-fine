{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dedc8bb-11ec-4d6b-8523-16c9ee95b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "\"\"\"\n",
    "COCO dataset which returns image_id for evaluation.\n",
    "\n",
    "Mostly copy-paste from https://github.com/pytorch/vision/blob/13b35ff/references/detection/coco_utils.py\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from pycocotools import mask as coco_mask\n",
    "\n",
    "# import datasets.transforms as T\n",
    "\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms, return_masks):\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "        self._transforms = transforms\n",
    "        self.prepare = ConvertCocoPolysToMask(return_masks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        img, target = self.prepare(img, target)\n",
    "        if self._transforms is not None:\n",
    "            img, target = self._transforms(img, target)\n",
    "        return img, target\n",
    "\n",
    "\n",
    "def convert_coco_poly_to_mask(segmentations, height, width):\n",
    "    masks = []\n",
    "    for polygons in segmentations:\n",
    "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
    "        mask = coco_mask.decode(rles)\n",
    "        if len(mask.shape) < 3:\n",
    "            mask = mask[..., None]\n",
    "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
    "        mask = mask.any(dim=2)\n",
    "        masks.append(mask)\n",
    "    if masks:\n",
    "        masks = torch.stack(masks, dim=0)\n",
    "    else:\n",
    "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
    "    return masks\n",
    "\n",
    "\n",
    "class ConvertCocoPolysToMask(object):\n",
    "    def __init__(self, return_masks=False):\n",
    "        self.return_masks = return_masks\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        w, h = image.size\n",
    "\n",
    "        image_id = target[\"image_id\"]\n",
    "        image_id = torch.tensor([image_id])\n",
    "\n",
    "        anno = target[\"annotations\"]\n",
    "\n",
    "        anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n",
    "\n",
    "        boxes = [obj[\"bbox\"] for obj in anno]\n",
    "\n",
    "        # guard against no boxes via resizing\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
    "        boxes[:, 2:] += boxes[:, :2]\n",
    "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
    "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
    "\n",
    "        classes = [obj[\"category_id\"] for obj in anno]\n",
    "        classes = torch.tensor(classes, dtype=torch.int64)\n",
    "\n",
    "        if self.return_masks:\n",
    "            segmentations = [obj[\"segmentation\"] for obj in anno]\n",
    "            masks = convert_coco_poly_to_mask(segmentations, h, w)\n",
    "\n",
    "        keypoints = None\n",
    "        if anno and \"keypoints\" in anno[0]:\n",
    "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
    "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "            num_keypoints = keypoints.shape[0]\n",
    "            if num_keypoints:\n",
    "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
    "\n",
    "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
    "        boxes = boxes[keep]\n",
    "        classes = classes[keep]\n",
    "        if self.return_masks:\n",
    "            masks = masks[keep]\n",
    "        if keypoints is not None:\n",
    "            keypoints = keypoints[keep]\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = classes\n",
    "        if self.return_masks:\n",
    "            target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        if keypoints is not None:\n",
    "            target[\"keypoints\"] = keypoints\n",
    "\n",
    "        # for conversion to coco api\n",
    "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
    "        iscrowd = torch.tensor([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno])\n",
    "        target[\"area\"] = area[keep]\n",
    "        target[\"iscrowd\"] = iscrowd[keep]\n",
    "\n",
    "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
    "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "# def make_coco_transforms(image_set):\n",
    "\n",
    "#     normalize = T.Compose([\n",
    "#         T.ToTensor(),\n",
    "#         T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "\n",
    "#     scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
    "\n",
    "#     if image_set == 'train':\n",
    "#         return T.Compose([\n",
    "#             T.RandomHorizontalFlip(),\n",
    "#             T.RandomSelect(\n",
    "#                 T.RandomResize(scales, max_size=1333),\n",
    "#                 T.Compose([\n",
    "#                     T.RandomResize([400, 500, 600]),\n",
    "#                     T.RandomSizeCrop(384, 600),\n",
    "#                     T.RandomResize(scales, max_size=1333),\n",
    "#                 ])\n",
    "#             ),\n",
    "#             normalize,\n",
    "#         ])\n",
    "\n",
    "#     if image_set == 'val':\n",
    "#         return T.Compose([\n",
    "#             T.RandomResize([800], max_size=1333),\n",
    "#             normalize,\n",
    "#         ])\n",
    "\n",
    "#     raise ValueError(f'unknown {image_set}')\n",
    "\n",
    "\n",
    "def build(image_set, args):\n",
    "    root = Path(args.coco_path)\n",
    "    assert root.exists(), f'provided COCO path {root} does not exist'\n",
    "    mode = 'coco'\n",
    "    PATHS = {\n",
    "        \"train\": (root / \"images\", root / f'{mode}.json'),\n",
    "        \"val\": (root / \"images\", root / f'val_{mode}.json'),\n",
    "    }\n",
    "\n",
    "    img_folder, ann_file = PATHS[image_set]\n",
    "    dataset = CocoDetection(img_folder, ann_file, transforms=None, return_masks=args.masks)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "f5ec1e97-b1f5-4443-aaeb-17f90fed659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "\"\"\"\n",
    "COCO dataset which returns image_id for evaluation.\n",
    "\n",
    "Mostly copy-paste from https://github.com/pytorch/vision/blob/13b35ff/references/detection/coco_utils.py\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from pycocotools import mask as coco_mask\n",
    "\n",
    "import datasets.transforms as T\n",
    "import albumentations as A\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "class ConstructionDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms, return_masks, dataset_type):\n",
    "        super(ConstructionDetection, self).__init__(img_folder, ann_file)\n",
    "        self._transforms = transforms\n",
    "        self.prepare = ConvertConstructionPolysToMask(return_masks)\n",
    "        self.dataset_type = dataset_type\n",
    "        \n",
    "    def get_random_set(self, idx):\n",
    "        collage_images = list(random.sample(range(len(self.ids)), 3))\n",
    "        \n",
    "        while idx in collage_images:\n",
    "            collage_images = list(random.sample(range(len(self.ids)), 3))\n",
    "        \n",
    "        collage_images.append(idx)\n",
    "        \n",
    "        targets = {i: [] for i in collage_images}\n",
    "        images = {i: [] for i in collage_images}\n",
    "        \n",
    "        return collage_images, targets, images\n",
    "        \n",
    "    def get_images(self, idx):\n",
    "        collage_images, targets, images = self.get_random_set(idx)\n",
    "        \n",
    "        for imid in collage_images:\n",
    "            image, target = super(ConstructionDetection, self).__getitem__(imid)\n",
    "            target = {'image_id': imid, 'annotations': target}\n",
    "            image, target = self.prepare(image, target)\n",
    "            targets[imid] = target\n",
    "            images[imid] = image\n",
    "            \n",
    "        return images, targets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _flip = flip_coin()\n",
    "        image_id = self.ids[idx]\n",
    "        \n",
    "        if _flip or self.dataset_type == 'val':\n",
    "            img, target = super(ConstructionDetection, self).__getitem__(idx)\n",
    "        else:\n",
    "            images, targets = self.get_images(idx)\n",
    "            img, target = prepare_collage(images, targets)\n",
    "        \n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        img, target = self.prepare(img, target)\n",
    "        if self._transforms is not None:\n",
    "            if self.dataset_type == 'val':\n",
    "                img, target = self._transforms(img, target)\n",
    "            elif _flip:\n",
    "                img, target = self._transforms[0](img, target)\n",
    "            else:\n",
    "                img, target = self._transforms[1](img, target)\n",
    "        return img, target\n",
    "\n",
    "    \n",
    "def flip_coin():\n",
    "    if torch.rand(1) > 1.0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def prepare_collage(imgs, targets):\n",
    "    idxs = imgs.keys()\n",
    "    \n",
    "    bbs = {i: [] for i in idxs}\n",
    "    cats = {i: [] for i in idxs}\n",
    "\n",
    "    collage_target = []\n",
    "\n",
    "    for i in idxs:\n",
    "        targets[i][\"boxes\"][:, 2:] -= targets[i][\"boxes\"][:, :2]\n",
    "        bbs[i] = targets[i][\"boxes\"].int().tolist()\n",
    "        cats[i]= targets[i][\"labels\"].int().tolist()\n",
    "\n",
    "    trans_imgs = []\n",
    "    trans_bbs = torch.tensor([])\n",
    "    trans_cats = []\n",
    "\n",
    "    transform = A.Compose(\n",
    "        [A.SmallestMaxSize(max_size=400), A.RandomCrop(width=300, height=300)],\n",
    "        bbox_params=A.BboxParams(format='coco', label_fields=['category_ids'], min_visibility=0.2),\n",
    "    )\n",
    "\n",
    "    for i in idxs:\n",
    "        image = np.array(imgs[i])\n",
    "\n",
    "        transformed = transform(image=image, bboxes=bbs[i], category_ids=cats[i])\n",
    "\n",
    "        trans_imgs.append(transformed)\n",
    "        bb_tensor = torch.tensor(transformed['bboxes'])\n",
    "\n",
    "        if len(bb_tensor) > 0:\n",
    "\n",
    "            if i == 1:\n",
    "                bb_tensor[:, 1]+=300\n",
    "            if i == 2:\n",
    "                bb_tensor[:, 0]+=300\n",
    "            if i == 3:\n",
    "                bb_tensor[:, 0]+=300\n",
    "                bb_tensor[:, 1]+=300\n",
    "\n",
    "            trans_bbs = torch.cat([trans_bbs, bb_tensor], dim=0)\n",
    "            trans_cats += transformed['category_ids']\n",
    "\n",
    "\n",
    "    collage_image = Image.fromarray(torch.cat([\n",
    "        torch.cat([\n",
    "            torch.tensor(trans_imgs[0]['image']), \n",
    "            torch.tensor(trans_imgs[1]['image'])\n",
    "        ], dim=0),\n",
    "        torch.cat([\n",
    "            torch.tensor(trans_imgs[2]['image']), \n",
    "            torch.tensor(trans_imgs[3]['image'])\n",
    "        ], dim=0)\n",
    "    ], dim=1).detach().numpy())\n",
    "\n",
    "    for bb, cid, ar in zip(trans_bbs, trans_cats, trans_bbs[:, 2] * trans_bbs[:, 3]):\n",
    "        collage_target.append({\n",
    "        'bbox': bb.tolist(),\n",
    "        'category_id': cid,\n",
    "        'area': ar\n",
    "    })\n",
    "    \n",
    "    return collage_image, collage_target\n",
    "\n",
    "\n",
    "def convert_construction_poly_to_mask(segmentations, height, width):\n",
    "    masks = []\n",
    "    for polygons in segmentations:\n",
    "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
    "        mask = coco_mask.decode(rles)\n",
    "        if len(mask.shape) < 3:\n",
    "            mask = mask[..., None]\n",
    "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
    "        mask = mask.any(dim=2)\n",
    "        masks.append(mask)\n",
    "    if masks:\n",
    "        masks = torch.stack(masks, dim=0)\n",
    "    else:\n",
    "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
    "    return masks\n",
    "\n",
    "\n",
    "class ConvertConstructionPolysToMask(object):\n",
    "    def __init__(self, return_masks=False):\n",
    "        self.return_masks = return_masks\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        w, h = image.size\n",
    "\n",
    "        image_id = target[\"image_id\"]\n",
    "        image_id = torch.tensor([image_id])\n",
    "\n",
    "        anno = target[\"annotations\"]\n",
    "\n",
    "        anno = [obj for obj in anno if 'iscrowd' not in obj or obj['iscrowd'] == 0]\n",
    "\n",
    "        boxes = [obj[\"bbox\"] for obj in anno]\n",
    "        # guard against no boxes via resizing\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
    "        boxes[:, 2:] += boxes[:, :2]\n",
    "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
    "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
    "\n",
    "        classes = [obj[\"category_id\"] for obj in anno]\n",
    "        classes = torch.tensor(classes, dtype=torch.int64)\n",
    "\n",
    "        if self.return_masks:\n",
    "            segmentations = [obj[\"segmentation\"] for obj in anno]\n",
    "            masks = convert_construction_poly_to_mask(segmentations, h, w)\n",
    "\n",
    "        keypoints = None\n",
    "        if anno and \"keypoints\" in anno[0]:\n",
    "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
    "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
    "            num_keypoints = keypoints.shape[0]\n",
    "            if num_keypoints:\n",
    "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
    "\n",
    "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
    "        boxes = boxes[keep]\n",
    "        classes = classes[keep]\n",
    "        if self.return_masks:\n",
    "            masks = masks[keep]\n",
    "        if keypoints is not None:\n",
    "            keypoints = keypoints[keep]\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = classes\n",
    "        if self.return_masks:\n",
    "            target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        if keypoints is not None:\n",
    "            target[\"keypoints\"] = keypoints\n",
    "\n",
    "        # for conversion to coco api\n",
    "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
    "        iscrowd = torch.tensor([obj[\"iscrowd\"] if \"iscrowd\" in obj else 0 for obj in anno])\n",
    "        target[\"area\"] = area[keep]\n",
    "        target[\"iscrowd\"] = iscrowd[keep]\n",
    "\n",
    "        target[\"orig_size\"] = torch.as_tensor([int(h), int(w)])\n",
    "        target[\"size\"] = torch.as_tensor([int(h), int(w)])\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "def make_construction_transforms(image_set):\n",
    "\n",
    "    normalize = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    scales = [480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800]\n",
    "\n",
    "    if image_set == 'train':\n",
    "        return T.Compose([\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomSelect(\n",
    "                T.RandomResize(scales, max_size=1333),\n",
    "                T.Compose([\n",
    "                    T.RandomResize([400, 500, 600]),\n",
    "                    T.RandomSizeCrop(384, 600),\n",
    "                    T.RandomResize(scales, max_size=1333),\n",
    "                ])\n",
    "            ),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    if image_set == 'val':\n",
    "        return T.Compose([\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "    raise ValueError(f'unknown {image_set}')\n",
    "\n",
    "\n",
    "def build(image_set, args):\n",
    "    root = Path(args.data_path)\n",
    "    assert root.exists(), f'provided Construction path {root} does not exist'\n",
    "    mode = 'coco'\n",
    "    PATHS = {\n",
    "        \"train\": (root / \"images\", root / f'aac_{mode}.json'),\n",
    "        \"val\": (root / \"images\", root / f'aac_val_{mode}.json'),\n",
    "    }\n",
    "\n",
    "    img_folder, ann_file = PATHS[image_set]\n",
    "    if image_set == 'train':\n",
    "        dataset = ConstructionDetection(img_folder, ann_file, transforms=(make_construction_transforms('train'), make_construction_transforms('val')), return_masks=args.masks, dataset_type=image_set)\n",
    "    else:\n",
    "        dataset = ConstructionDetection(img_folder, ann_file, transforms=make_construction_transforms(image_set), return_masks=args.masks, dataset_type=image_set)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "092e759b-d45f-4a6a-a9c4-bc221dd480f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python36.zip', '/usr/lib/python3.6', '/usr/lib/python3.6/lib-dynload', '', '/home/ammar/projects/.venv/lib/python3.6/site-packages', '/home/ammar/projects/.venv/lib/python3.6/site-packages/IPython/extensions', '/home/ammar/.ipython', '/home/ammar/projects/construction/detr-fine/detr/']\n"
     ]
    }
   ],
   "source": [
    "import sys    \n",
    "import os\n",
    "\n",
    "if os.path.join(os.getcwd(), \"detr/\") not in sys.path:\n",
    "    sys.path.append(os.path.join(os.getcwd(), \"detr/\"))\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e06f7515-6fec-4116-b572-4ac9385d594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from panopticapi.utils import rgb2id\n",
    "# from util.box_ops import masks_to_boxes\n",
    "\n",
    "from datasets.construction import make_construction_transforms\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "def box_xywh_to_xyxy(x):\n",
    "    xs, ys, w, h = x.unbind(-1)\n",
    "    b = [xs, ys, (xs + w), (ys + h)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "\n",
    "def masks_to_boxes(segments):\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    iscrowd = []\n",
    "    area = []\n",
    "    ids = []\n",
    "\n",
    "    for ann in segments:\n",
    "        if len(ann[\"bbox\"]) == 4:\n",
    "            boxes.append(ann[\"bbox\"])\n",
    "            area.append(ann['area'])\n",
    "        else:\n",
    "            boxes.append([0, 0, 2, 2])\n",
    "            area.append(4)\n",
    "        labels.append(ann[\"category_id\"])\n",
    "        iscrowd.append(ann['iscrowd'])\n",
    "        ids.append(ann['id'])\n",
    "    \n",
    "    if len(boxes) == 0 and len(labels) == 0:\n",
    "        boxes.append([0, 0, 2, 2])\n",
    "        labels.append(1)\n",
    "        area.append(4)\n",
    "        iscrowd.append(0)\n",
    "\n",
    "    boxes = torch.tensor(boxes, dtype=torch.int64)\n",
    "    labels = torch.tensor(labels, dtype=torch.int64)\n",
    "    iscrowd = torch.tensor(iscrowd)\n",
    "    area = torch.tensor(area)\n",
    "    boxes = box_xywh_to_xyxy(boxes)\n",
    "    return boxes, labels, iscrowd, area, ids\n",
    "\n",
    "class ConstructionPanoptic:\n",
    "    def __init__(self, img_folder, ann_folder, ann_file, transforms=None, return_masks=True):\n",
    "        with open(ann_file, \"r\") as f:\n",
    "            self.coco = json.load(f)\n",
    "\n",
    "        # sort 'images' field so that they are aligned with 'annotations'\n",
    "        # i.e., in alphabetical order\n",
    "        self.coco[\"images\"] = sorted(self.coco[\"images\"], key=lambda x: x[\"id\"])\n",
    "        # sanity check\n",
    "        if \"annotations\" in self.coco:\n",
    "            for img, ann in zip(self.coco[\"images\"], self.coco[\"annotations\"]):\n",
    "                assert img[\"file_name\"][:-4] == ann[\"file_name\"][:-4]\n",
    "\n",
    "        self.img_folder = img_folder\n",
    "        self.ann_folder = ann_folder\n",
    "        self.ann_file = ann_file\n",
    "        self.transforms = transforms\n",
    "        self.return_masks = return_masks\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            ann_info = (\n",
    "                self.coco[\"annotations\"][idx]\n",
    "                if \"annotations\" in self.coco\n",
    "                else self.coco[\"images\"][idx]\n",
    "            )\n",
    "            img_path = Path(self.img_folder) / ann_info[\"file_name\"].replace(\".png\", \".jpg\")\n",
    "            ann_path = Path(self.ann_folder) / ann_info[\"file_name\"]\n",
    "            \n",
    "            print(ann_info[\"file_name\"])\n",
    "\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            w, h = img.size\n",
    "            if \"segments_info\" in ann_info:\n",
    "                masks = np.asarray(Image.open(ann_path), dtype=np.uint32)\n",
    "                masks = rgb2id(masks)\n",
    "\n",
    "                ids = np.array([ann[\"id\"] for ann in ann_info[\"segments_info\"]])\n",
    "                print(ids)\n",
    "                masks = masks == ids[:, None, None]\n",
    "\n",
    "                masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "                \n",
    "                # labels = torch.tensor(\n",
    "                #     [ann[\"category_id\"] for ann in ann_info[\"segments_info\"]],\n",
    "                #     dtype=torch.int64,\n",
    "                # )\n",
    "\n",
    "\n",
    "            target = {}\n",
    "            target['image_id'] = torch.tensor([ann_info['image_id'] if \"image_id\" in ann_info else ann_info[\"id\"]])\n",
    "            if self.return_masks:\n",
    "                target['masks'] = masks\n",
    "\n",
    "            boxes, labels, iscrowd, area, ids = masks_to_boxes(ann_info[\"segments_info\"])\n",
    "            \n",
    "            print(ids)\n",
    "\n",
    "            target['labels'] = labels\n",
    "\n",
    "            # Instead of finding boxes, just take the one from json info available \n",
    "            # target[\"boxes\"] = masks_to_boxes(ann_info[\"segments_info\"])\n",
    "            target[\"boxes\"] = boxes\n",
    "\n",
    "\n",
    "            target['size'] = torch.as_tensor([int(h), int(w)])\n",
    "            target['orig_size'] = torch.as_tensor([int(h), int(w)])\n",
    "\n",
    "            target['iscrowd'] = iscrowd\n",
    "            target['area'] = area\n",
    "            # if \"segments_info\" in ann_info:\n",
    "            #     for name in ['iscrowd', 'area']:\n",
    "            #         target[name] = torch.tensor([ann[name] for ann in ann_info['segments_info']])\n",
    "\n",
    "            if self.transforms is not None:\n",
    "                img, target = self.transforms(img, target)\n",
    "\n",
    "            return img, target\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(ann_info)\n",
    "            raise e\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco['images'])\n",
    "\n",
    "    def get_height_and_width(self, idx):\n",
    "        img_info = self.coco['images'][idx]\n",
    "        height = img_info['height']\n",
    "        width = img_info['width']\n",
    "        return height, width\n",
    "\n",
    "\n",
    "def build(image_set, args):\n",
    "    root = Path(args.data_path)\n",
    "    assert (\n",
    "        root.exists()\n",
    "    ), f\"provided Panoptic path {root} does not exist\"\n",
    "\n",
    "    mode = \"panoptic\"\n",
    "\n",
    "    PATHS = {\n",
    "        \"train\": (\"images\", f\"{mode}\", f\"{mode}.json\"),\n",
    "        \"val\": (\"images\", f\"val_{mode}\", f\"val_{mode}.json\"),\n",
    "    }\n",
    "\n",
    "    img_folder, ann_folder, ann_file = PATHS[image_set]\n",
    "    img_folder_path = root / img_folder\n",
    "    ann_folder_path = root / ann_folder\n",
    "    ann_file = root / ann_file\n",
    "\n",
    "    dataset = ConstructionPanoptic(\n",
    "        img_folder_path,\n",
    "        ann_folder_path,\n",
    "        ann_file,\n",
    "        transforms=make_construction_transforms(image_set),\n",
    "        return_masks=args.masks,\n",
    "    )\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b10d98f2-4c7f-4cf0-9801-fa34e5d4adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    pass\n",
    "\n",
    "args = Args()\n",
    "\n",
    "args.data_path = '../data'\n",
    "args.masks = False\n",
    "args.batch_size = 5\n",
    "args.num_workers = 4\n",
    "args.masks = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d8d8c11-1706-4ee6-8d1c-3c2d47a1c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import util.misc as utils\n",
    "\n",
    "train_dataset = build('train', args)\n",
    "sampler_train = torch.utils.data.RandomSampler(train_dataset)\n",
    "\n",
    "batch_sampler_train = torch.utils.data.BatchSampler(sampler_train, args.batch_size, drop_last=True)\n",
    "\n",
    "data_loader_train = DataLoader(train_dataset, batch_sampler=batch_sampler_train, collate_fn=utils.collate_fn, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e8bba93-c2d7-444e-be81-93adcf14c2f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aac_blocks_2176.png\n",
      "[   33265 11881084  2100087  1120607]\n",
      "[33265, 11881084, 2100087, 1120607]\n",
      "torch.Size([4, 672, 895])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'image_id': tensor([2176]),\n",
       " 'masks': tensor([[[ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True],\n",
       "          [ True,  True,  True,  ...,  True,  True,  True],\n",
       "          ...,\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False]],\n",
       " \n",
       "         [[False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False]],\n",
       " \n",
       "         [[False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False]],\n",
       " \n",
       "         [[False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          [False, False, False,  ..., False, False, False],\n",
       "          ...,\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True],\n",
       "          [False, False, False,  ...,  True,  True,  True]]]),\n",
       " 'labels': tensor([ 7,  9, 17, 17]),\n",
       " 'boxes': tensor([[0.5000, 0.5000, 1.0000, 1.0000],\n",
       "         [0.8544, 0.6071, 0.2322, 0.7858],\n",
       "         [0.4613, 0.5555, 0.9226, 0.8890],\n",
       "         [0.9717, 0.5539, 0.0566, 0.8923]]),\n",
       " 'size': tensor([672, 895]),\n",
       " 'orig_size': tensor([620, 997]),\n",
       " 'iscrowd': tensor([0, 0, 0, 0]),\n",
       " 'area': tensor([601440.0000, 109732.0312, 493265.4375,  30348.2070])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for data in data_loader_train:\n",
    "#     pass\n",
    "\n",
    "img, target = train_dataset.__getitem__(2173)\n",
    "\n",
    "print(target['masks'].size())\n",
    "\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5bda1e69-0688-48c5-9043-483efdfbc190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:222\n"
     ]
    }
   ],
   "source": [
    "logging.warning(222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "e5bfa65d-c83e-4eca-b459-62d1a4b695e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317856.0"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1032.0000 * 308.0000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd3d26e7-d062-4d95-95cc-4f700b45e8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "def convert(o):\n",
    "    if isinstance(o, np.generic): return o.item()  \n",
    "    raise TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b595b9d1-13b9-4157-a1b4-17428370f2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Files\n",
    "\n",
    "ROOT_DIR = './data'\n",
    "\n",
    "COCO_FILE = 'coco.json'\n",
    "PANOPTIC_FILE = 'panoptic.json'\n",
    "\n",
    "AAC_COCO_FILE = 'aac_coco.json'\n",
    "AAC_PANOPTIC_FILE = 'aac_panoptic.json'\n",
    "\n",
    "VAL_COCO_FILE = 'val_coco.json'\n",
    "VAL_PANOPTIC_FILE = 'val_panoptic.json'\n",
    "\n",
    "AAC_VAL_COCO_FILE = 'aac_val_coco.json'\n",
    "AAC_VAL_PANOPTIC_FILE = 'aac_val_panoptic.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef674d90-d114-4524-a7cb-d73486b35da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Prepare only one class DS\n",
    "\n",
    "# with open(os.path.join(ROOT_DIR, COCO_FILE), \"r\") as coco_file:\n",
    "#     coco_data = json.load(coco_file)\n",
    "    \n",
    "# annotations = []\n",
    "# images_ids = []\n",
    "# images = []\n",
    "\n",
    "# # Remove those images where we dont have any segmentations\n",
    "# for img in coco_data[\"images\"]:\n",
    "#     if \"aac_blocks\" in img[\"file_name\"]:\n",
    "#         images.append(img)\n",
    "#         images_ids.append(img['id'])\n",
    "\n",
    "# for ann in coco_data[\"annotations\"]:\n",
    "#     if ann['image_id'] in images_ids:\n",
    "#         annotations.append(ann)\n",
    "\n",
    "# coco_data[\"images\"] = images\n",
    "# coco_data[\"annotations\"] = annotations\n",
    "\n",
    "# with open(os.path.join(ROOT_DIR, AAC_COCO_FILE), \"w\") as aac_coco_file:\n",
    "#     json.dump(coco_data, aac_coco_file) # , default=convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f990aae-c120-4cbb-bcbe-640e6d765a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Prepare only one class DS\n",
    "\n",
    "# with open(os.path.join(ROOT_DIR, VAL_COCO_FILE), \"r\") as val_coco_file:\n",
    "#     val_coco_data = json.load(val_coco_file)\n",
    "    \n",
    "# annotations = []\n",
    "# images_ids = []\n",
    "# images = []\n",
    "\n",
    "# # Remove those images where we dont have any segmentations\n",
    "# for img in val_coco_data[\"images\"]:\n",
    "#     if \"aac_blocks\" in img[\"file_name\"]:\n",
    "#         images.append(img)\n",
    "#         images_ids.append(img['id'])\n",
    "\n",
    "# for ann in val_coco_data[\"annotations\"]:\n",
    "#     if ann['image_id'] in images_ids:\n",
    "#         annotations.append(ann)\n",
    "\n",
    "# val_coco_data[\"images\"] = images\n",
    "# val_coco_data[\"annotations\"] = annotations\n",
    "\n",
    "# with open(os.path.join(ROOT_DIR, AAC_VAL_COCO_FILE), \"w\") as aac_val_coco_file:\n",
    "#     json.dump(val_coco_data, aac_val_coco_file) #, default=convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "238cfc89-6dab-477a-8f2b-4441b2395106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rename iscroud to iscrowd\n",
    "\n",
    "# with open(os.path.join(ROOT_DIR, AAC_VAL_COCO_FILE), \"r\") as aac_val_coco_file:\n",
    "#     aac_val_coco_data = json.load(aac_val_coco_file)\n",
    "    \n",
    "# for ann in aac_val_coco_data['annotations']:\n",
    "#     if 'iscroud' in ann:\n",
    "#         ann['iscrowd'] = ann['iscroud']\n",
    "        \n",
    "# with open(os.path.join(ROOT_DIR, AAC_VAL_COCO_FILE), \"w\") as aac_val_coco_file:\n",
    "#     json.dump(aac_val_coco_data, aac_val_coco_file) #, default=convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8c7aee0-34c6-418e-ab4e-efda34d393cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rename iscroud to iscrowd\n",
    "\n",
    "# with open(os.path.join(ROOT_DIR, AAC_COCO_FILE), \"r\") as aac_coco_file:\n",
    "#     aac_coco_data = json.load(aac_coco_file)\n",
    "    \n",
    "# for ann in aac_coco_data['annotations']:\n",
    "#     if 'iscroud' in ann:\n",
    "#         ann['iscrowd'] = ann['iscroud']\n",
    "        \n",
    "# with open(os.path.join(ROOT_DIR, AAC_COCO_FILE), \"w\") as aac_coco_file:\n",
    "#     json.dump(aac_coco_data, aac_coco_file) #, default=convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d47b9-0ce8-4d56-972d-b256dc374cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects-env",
   "language": "python",
   "name": "projects-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
