{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cu111\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# The GPU id to use, \"0\" to  \"7\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'detr' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/detr.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), \"detr/\"))\n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> tuple:\n",
    "    \"\"\"Get Device type\n",
    "\n",
    "    Returns:\n",
    "        tuple: Device type\n",
    "    \"\"\"\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    return (use_cuda, device)\n",
    "\n",
    "use_cuda, device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "\n",
    "from categories_meta import COCO_CATEGORIES, COCO_NAMES\n",
    "from panopticapi.utils import id2rgb, rgb2id\n",
    "import panopticapi\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import requests\n",
    "import json\n",
    "import io\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "# Create Original Segmented Image\n",
    "import overlay_custom_mask\n",
    "import convert_to_coco\n",
    "\n",
    "from categories_meta import COCO_CATEGORIES, NEW_CATEGORIES, MAPPINGS, INFO, LICENSES, cat2id, id2cat\n",
    "import coco_creator_tools\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load detr model\n",
    "model, postprocessor = torch.hub.load('detr', 'detr_resnet101_panoptic', source='local', pretrained=True, return_postprocessor=True, num_classes=250)\n",
    "# Convert to eval mode\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "\n",
    "# category_paths = glob.glob('/home/ammar/data/construction/*')\n",
    "\n",
    "category_paths = [\n",
    "    '/home/ammar/data/construction/hydra_crane',\n",
    "    '/home/ammar/data/construction/split_units',\n",
    "    '/home/ammar/data/construction/wheel_loader',\n",
    "    '/home/ammar/data/construction/dump_truck_tipper_truck',\n",
    "    '/home/ammar/data/construction/transit_mixer',\n",
    "    '/home/ammar/data/construction/glass_wool',\n",
    "    '/home/ammar/data/construction/rmu_units',\n",
    "    '/home/ammar/data/construction/chiller',\n",
    "    '/home/ammar/data/construction/adhesives',\n",
    "    '/home/ammar/data/construction/structural_steel_channel',\n",
    "    '/home/ammar/data/construction/sanitary_fixtures',\n",
    "    '/home/ammar/data/construction/aac_blocks',\n",
    "    '/home/ammar/data/construction/control_panel',\n",
    "    '/home/ammar/data/construction/vrf_units',\n",
    "    '/home/ammar/data/construction/wood_primer',\n",
    "    '/home/ammar/data/construction/rcc_hume_pipes',\n",
    "    '/home/ammar/data/construction/hoist',\n",
    "    '/home/ammar/data/construction/emulsion_paint',\n",
    "    '/home/ammar/data/construction/threaded_rod',\n",
    "    '/home/ammar/data/construction/switch_boards_and_switches',\n",
    "    '/home/ammar/data/construction/smoke_detectors',\n",
    "    '/home/ammar/data/construction/vcb_panel',\n",
    "    '/home/ammar/data/construction/rmc_batching_plant',\n",
    "    '/home/ammar/data/construction/fine_aggregate',\n",
    "    '/home/ammar/data/construction/hollow_concrete_blocks',\n",
    "    '/home/ammar/data/construction/junction_box',\n",
    "    '/home/ammar/data/construction/marble',\n",
    "    '/home/ammar/data/construction/pipe_fittings',\n",
    "    '/home/ammar/data/construction/lime',\n",
    "    '/home/ammar/data/construction/vitrified_tiles',\n",
    "    '/home/ammar/data/construction/water_tank',\n",
    "    '/home/ammar/data/construction/concrete_mixer_machine',\n",
    "    '/home/ammar/data/construction/metal_primer',\n",
    "    '/home/ammar/data/construction/river_sand',\n",
    "    '/home/ammar/data/construction/distribution_transformer',\n",
    "    '/home/ammar/data/construction/fire_buckets',\n",
    "    '/home/ammar/data/construction/concrete_pump',\n",
    "    '/home/ammar/data/construction/refrigerant_gas',\n",
    "    '/home/ammar/data/construction/grader',\n",
    "    '/home/ammar/data/construction/ahus',\n",
    "    '/home/ammar/data/construction/interlocked_switched_socket',\n",
    "    '/home/ammar/data/construction/skid_steer_loader',\n",
    "    '/home/ammar/data/construction/enamel_paint',\n",
    "    '/home/ammar/data/construction/texture_paint',\n",
    "    '/home/ammar/data/construction/fire_extinguishers',\n",
    "    '/home/ammar/data/construction/aluminium_frames_for_false_ceiling',\n",
    "    '/home/ammar/data/construction/hot_mix_plant',\n",
    "    '/home/ammar/data/construction/cu_piping'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(o):\n",
    "    if isinstance(o, np.generic): return o.item()  \n",
    "    raise TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category: hydra_crane\n",
      "Completed Category: hydra_crane, Time Taken: 0:00:47\n",
      "101 416 418\n",
      "Processing Category: split_units\n",
      "Completed Category: split_units, Time Taken: 0:05:40\n",
      "639 2083 2097\n",
      "Processing Category: wheel_loader\n",
      "Completed Category: wheel_loader, Time Taken: 0:01:27\n",
      "789 2777 2799\n",
      "Processing Category: dump_truck_tipper_truck\n",
      "Completed Category: dump_truck_tipper_truck, Time Taken: 0:00:47\n",
      "889 3263 3285\n",
      "Processing Category: transit_mixer\n",
      "Completed Category: transit_mixer, Time Taken: 0:00:27\n",
      "939 3512 3534\n",
      "Processing Category: glass_wool\n",
      "Completed Category: glass_wool, Time Taken: 0:00:37\n",
      "1009 3820 3842\n",
      "Processing Category: rmu_units\n",
      "Completed Category: rmu_units, Time Taken: 0:00:33\n",
      "1109 4065 4088\n",
      "Processing Category: chiller\n",
      "Completed Category: chiller, Time Taken: 0:00:22\n",
      "1154 4215 4242\n",
      "Processing Category: adhesives\n",
      "Completed Category: adhesives, Time Taken: 0:00:34\n",
      "1254 4423 4450\n",
      "Processing Category: structural_steel_channel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ammar/projects/.venv/lib/python3.6/site-packages/PIL/Image.py:963: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  \"Palette images with Transparency expressed in bytes should be \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Category: structural_steel_channel, Time Taken: 0:02:38\n",
      "1655 5320 5348\n",
      "Processing Category: sanitary_fixtures\n",
      "Completed Category: sanitary_fixtures, Time Taken: 0:07:28\n",
      "2176 7990 8020\n",
      "Processing Category: aac_blocks\n",
      "Completed Category: aac_blocks, Time Taken: 0:03:20\n",
      "2439 9026 9057\n",
      "Processing Category: control_panel\n",
      "Completed Category: control_panel, Time Taken: 0:02:10\n",
      "2684 9714 9758\n",
      "Processing Category: vrf_units\n",
      "Completed Category: vrf_units, Time Taken: 0:00:33\n",
      "2734 9836 9880\n",
      "Processing Category: wood_primer\n",
      "Completed Category: wood_primer, Time Taken: 0:00:05\n",
      "2746 9865 9910\n",
      "Processing Category: rcc_hume_pipes\n",
      "Completed Category: rcc_hume_pipes, Time Taken: 0:02:21\n",
      "2986 10577 10625\n",
      "Processing Category: hoist\n",
      "Completed Category: hoist, Time Taken: 0:06:14\n",
      "3521 12571 12641\n",
      "Processing Category: emulsion_paint\n",
      "Completed Category: emulsion_paint, Time Taken: 0:00:16\n",
      "3553 12650 12722\n",
      "Processing Category: threaded_rod\n",
      "Error occurred while processig file: /home/ammar/data/construction/threaded_rod/images/img_052.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-30-d082edade9da>\", line 72, in <module>\n",
      "    imo = Image.open(processing_file).convert('RGB')\n",
      "  File \"/home/ammar/projects/.venv/lib/python3.6/site-packages/PIL/Image.py\", line 2912, in open\n",
      "    fp = builtins.open(filename, \"rb\")\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/ammar/data/construction/threaded_rod/images/img_052.jpg'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while processig file: /home/ammar/data/construction/threaded_rod/images/img_050.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-30-d082edade9da>\", line 72, in <module>\n",
      "    imo = Image.open(processing_file).convert('RGB')\n",
      "  File \"/home/ammar/projects/.venv/lib/python3.6/site-packages/PIL/Image.py\", line 2912, in open\n",
      "    fp = builtins.open(filename, \"rb\")\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/ammar/data/construction/threaded_rod/images/img_050.jpg'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Category: threaded_rod, Time Taken: 0:18:18\n",
      "4104 13989 14061\n",
      "Processing Category: switch_boards_and_switches\n",
      "Completed Category: switch_boards_and_switches, Time Taken: 0:03:22\n",
      "4604 15025 15130\n",
      "Processing Category: smoke_detectors\n",
      "Completed Category: smoke_detectors, Time Taken: 0:00:30\n",
      "4654 15119 15224\n",
      "Processing Category: vcb_panel\n",
      "Completed Category: vcb_panel, Time Taken: 0:04:55\n",
      "5154 16808 16934\n",
      "Processing Category: rmc_batching_plant\n",
      "Completed Category: rmc_batching_plant, Time Taken: 0:00:30\n",
      "5213 17108 17239\n",
      "Processing Category: fine_aggregate\n",
      "Completed Category: fine_aggregate, Time Taken: 0:17:13\n",
      "5713 18088 18221\n",
      "Processing Category: hollow_concrete_blocks\n",
      "Completed Category: hollow_concrete_blocks, Time Taken: 0:00:19\n",
      "5763 18248 18382\n",
      "Processing Category: junction_box\n",
      "Completed Category: junction_box, Time Taken: 0:00:21\n",
      "5815 18388 18523\n",
      "Processing Category: marble\n",
      "Completed Category: marble, Time Taken: 0:00:24\n",
      "5865 18573 18711\n",
      "Processing Category: pipe_fittings\n",
      "Completed Category: pipe_fittings, Time Taken: 0:00:15\n",
      "5919 18689 18827\n",
      "Processing Category: lime\n",
      "Completed Category: lime, Time Taken: 0:07:26\n",
      "6431 21091 21232\n",
      "Processing Category: vitrified_tiles\n",
      "Completed Category: vitrified_tiles, Time Taken: 0:03:01\n",
      "6831 22622 22799\n",
      "Processing Category: water_tank\n",
      "Completed Category: water_tank, Time Taken: 0:02:15\n",
      "7135 23993 24173\n",
      "Processing Category: concrete_mixer_machine\n",
      "Completed Category: concrete_mixer_machine, Time Taken: 0:00:32\n",
      "7185 24169 24350\n",
      "Processing Category: metal_primer\n",
      "Completed Category: metal_primer, Time Taken: 0:00:21\n",
      "7244 24290 24471\n",
      "Processing Category: river_sand\n",
      "Completed Category: river_sand, Time Taken: 0:05:44\n",
      "7414 25154 25336\n",
      "Processing Category: distribution_transformer\n",
      "Completed Category: distribution_transformer, Time Taken: 0:19:30\n",
      "7825 26970 27155\n",
      "Processing Category: fire_buckets\n",
      "Completed Category: fire_buckets, Time Taken: 0:04:17\n",
      "8283 28423 28615\n",
      "Processing Category: concrete_pump\n",
      "Completed Category: concrete_pump, Time Taken: 0:00:37\n",
      "8359 28736 28928\n",
      "Processing Category: refrigerant_gas\n",
      "Completed Category: refrigerant_gas, Time Taken: 0:00:41\n",
      "8457 29065 29258\n",
      "Processing Category: grader\n",
      "Completed Category: grader, Time Taken: 0:03:02\n",
      "8757 30468 30667\n",
      "Processing Category: ahus\n",
      "Completed Category: ahus, Time Taken: 0:00:43\n",
      "8890 30832 31037\n",
      "Processing Category: interlocked_switched_socket\n",
      "Completed Category: interlocked_switched_socket, Time Taken: 0:00:29\n",
      "8990 30988 31196\n",
      "Processing Category: skid_steer_loader\n",
      "Error occurred while processig file: /home/ammar/data/construction/skid_steer_loader/images/img_105.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-30-d082edade9da>\", line 72, in <module>\n",
      "    imo = Image.open(processing_file).convert('RGB')\n",
      "  File \"/home/ammar/projects/.venv/lib/python3.6/site-packages/PIL/Image.py\", line 2912, in open\n",
      "    fp = builtins.open(filename, \"rb\")\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/ammar/data/construction/skid_steer_loader/images/img_105.jpg'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Category: skid_steer_loader, Time Taken: 0:01:44\n",
      "9166 31877 32092\n",
      "Processing Category: enamel_paint\n",
      "Completed Category: enamel_paint, Time Taken: 0:00:19\n",
      "9217 31997 32214\n",
      "Processing Category: texture_paint\n",
      "Completed Category: texture_paint, Time Taken: 0:00:19\n",
      "9269 32100 32319\n",
      "Processing Category: fire_extinguishers\n",
      "Completed Category: fire_extinguishers, Time Taken: 0:01:36\n",
      "9497 32726 32948\n",
      "Processing Category: aluminium_frames_for_false_ceiling\n",
      "Completed Category: aluminium_frames_for_false_ceiling, Time Taken: 0:00:26\n",
      "9547 32868 33091\n",
      "Processing Category: hot_mix_plant\n",
      "Completed Category: hot_mix_plant, Time Taken: 0:00:57\n",
      "9647 33342 33570\n",
      "Processing Category: cu_piping\n",
      "Completed Category: cu_piping, Time Taken: 0:03:03\n",
      "10147 34720 34955\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = './data'\n",
    "\n",
    "processing_file = \"\"\n",
    "processing_data = []\n",
    "\n",
    "image_id = 1\n",
    "annotation_id = 1\n",
    "segment_id = 1\n",
    "\n",
    "GLOBAL_COCO = {\n",
    "    \"licenses\": LICENSES,\n",
    "    \"info\": INFO,\n",
    "    \"categories\": NEW_CATEGORIES,\n",
    "    \"annotations\": [],\n",
    "    \"images\": []\n",
    "}\n",
    "\n",
    "GLOBAL_PANOPTIC = {\n",
    "    \"licenses\": LICENSES,\n",
    "    \"info\": INFO,\n",
    "    \"categories\": NEW_CATEGORIES,\n",
    "    \"annotations\": [],\n",
    "    \"images\": []\n",
    "}\n",
    "\n",
    "############################ Create DATASET ################################\n",
    "\n",
    "# run through all folders in dataset\n",
    "for category_path in category_paths:\n",
    "    # store starting time\n",
    "    start = time.time()\n",
    "    # get category name\n",
    "    category_name = category_path.split(\"/\")[5]\n",
    "    print(\"Processing Category:\", category_name)\n",
    "    # open category coco file\n",
    "    with open(os.path.join(category_path, \"coco.json\"), \"r\") as coco_file:\n",
    "        category_coco = json.load(coco_file)\n",
    "        \n",
    "    images_root = os.path.join(category_path, 'images')\n",
    "        \n",
    "    # Process all images\n",
    "    ## 1. Create a temp json which contains each image and its annotations\n",
    "    ## 2. Run over this list\n",
    "    ### 1. Copy this image as .jpg in GLOBAL_DIR\n",
    "    ### 2. Find all segments for this image\n",
    "    ### 3. Create new anotation segment which includes annotations from custom classes\n",
    "    \n",
    "    TEMP_COCO_IMAGES = {}\n",
    "    \n",
    "    # Run over all images\n",
    "    for im in category_coco[\"images\"]:\n",
    "        im['annotations'] = []\n",
    "        TEMP_COCO_IMAGES[im['id']] = im\n",
    "        \n",
    "    for ann in category_coco[\"annotations\"]:\n",
    "        TEMP_COCO_IMAGES[ann['image_id']][\"annotations\"].append(ann)\n",
    "        \n",
    "    for i, image_coco in TEMP_COCO_IMAGES.items():\n",
    "        # get image path\n",
    "        ## This data can be used further for logging if failed while processing\n",
    "        processing_file = os.path.join(images_root, image_coco['file_name'])\n",
    "        processing_data = image_coco\n",
    "        output_file_name = category_name + \"_\" + str(image_id) + \".jpg\"\n",
    "        output_file_path = os.path.join(ROOT_DIR, \"images\", output_file_name)\n",
    "        \n",
    "        output_mask_name = category_name + \"_\" + str(image_id) + \".png\"\n",
    "        output_mask_path = os.path.join(ROOT_DIR, \"masks\", output_mask_name)\n",
    "        \n",
    "        try:\n",
    "\n",
    "            # Read this image and get shape of image\n",
    "            imo = Image.open(processing_file).convert('RGB')\n",
    "\n",
    "            try:\n",
    "                h, w, c = np.array(imo).shape\n",
    "            except:\n",
    "                h, w = np.array(imo).shape\n",
    "                c = 1\n",
    "\n",
    "            # if no of channels != 3, open the image and convert it to 3 channel - RGB\n",
    "            if c == 4 or c == 1:\n",
    "                imo = imo.convert('RGB')\n",
    "                h, w, c = np.array(imo).shape\n",
    "\n",
    "            # Create a copy of image this will be used for further processing\n",
    "            im = imo.copy()\n",
    "\n",
    "            # Apply transform and convert image to batch\n",
    "            # mean-std normalize the input image (batch-size: 1)\n",
    "            img = transform(im).unsqueeze(0).to(device)  # [h, w, c] -> [1, c, ht, wt]\n",
    "\n",
    "            # Generate output for image\n",
    "            out = model(img)\n",
    "\n",
    "            # Generate score\n",
    "            # compute the scores, excluding the \"no-object\" class (the last one)\n",
    "            scores = out[\"pred_logits\"].softmax(-1)[..., :-1].max(-1)[0]\n",
    "\n",
    "            # threshold the confidence\n",
    "            keep = scores > 0.85\n",
    "\n",
    "            # Keep only ones above threshold\n",
    "            pred_logits, pred_boxes = out[\"pred_logits\"][keep][:, :len(\n",
    "                COCO_NAMES) - 1], out[\"pred_boxes\"][keep]\n",
    "\n",
    "            # the post-processor expects as input the target size of the predictions (which we set here to the image size)\n",
    "            result = postprocessor(out, torch.as_tensor(img.shape[-2:]).unsqueeze(0))[0]\n",
    "\n",
    "            # The segmentation is stored in a special-format png\n",
    "            panoptic_seg = Image.open(io.BytesIO(result['png_string'])).resize((w, h), Image.NEAREST)\n",
    "            # (wp, hp) = panoptic_seg.size\n",
    "            panoptic_seg = np.array(panoptic_seg, dtype=np.uint8).copy()\n",
    "\n",
    "            # We retrieve the ids corresponding to each mask\n",
    "            panoptic_seg_id = rgb2id(panoptic_seg)\n",
    "            \n",
    "            ## Merge predicted annotations\n",
    "            \n",
    "            # 1. Get Mapping from predicted id to new ids\n",
    "            ## 1. Merge segment ids\n",
    "            unique_category_id = []\n",
    "            for i, segment in enumerate(result['segments_info']):\n",
    "                result['segments_info'][i][\"category_id\"] = MAPPINGS[result['segments_info'][i][\"category_id\"]]\n",
    "                if result['segments_info'][i][\"category_id\"] not in unique_category_id:\n",
    "                    unique_category_id.append(result['segments_info'][i][\"category_id\"])\n",
    "            \n",
    "            # Sort array\n",
    "            unique_category_id.sort()\n",
    "            \n",
    "            unique_category_id_to_id =  {category_id: i for i, category_id in enumerate(unique_category_id)}\n",
    "            unique_id_to_category_id =  {i: category_id for category_id, i in unique_category_id_to_id.items()}\n",
    "            \n",
    "            for i, segment in enumerate(result['segments_info']):\n",
    "                result['segments_info'][i][\"new_id\"] = unique_category_id_to_id[result['segments_info'][i][\"category_id\"]]\n",
    "            \n",
    "            # Update original panoptic_seg_id array with new ids as the new segmentation combines different categories.\n",
    "            custom_panoptic_seg_id = np.zeros((panoptic_seg_id.shape[0], panoptic_seg_id.shape[1]), dtype=np.uint8)\n",
    "            \n",
    "            # Update this custom panoptic seg matrix\n",
    "            for i, segment in enumerate(result['segments_info']):\n",
    "                custom_panoptic_seg_id[result['segments_info'][i]['id'] == panoptic_seg_id] = result['segments_info'][i]['new_id']\n",
    "                \n",
    "            # Create new Segmentation info\n",
    "            # [{'area': 243, 'category_id': 3, 'id': 0, 'isthing': True},\n",
    "            #   {'area': 730578, 'category_id': 184, 'id': 1, 'isthing': False}]\n",
    "            \n",
    "            custom_panoptic_segments_info = []\n",
    "            for category_id in unique_category_id:\n",
    "                custom_panoptic_segments_info.append({\n",
    "                    'segment_id': unique_category_id_to_id[category_id], \n",
    "                    'category_id': category_id,\n",
    "                    'bbox': [],\n",
    "                    'area': 0,\n",
    "                    'iscroud': 0,\n",
    "                    'isthing': 0\n",
    "                })\n",
    "\n",
    "            # annotations of our construction things\n",
    "            omask = processing_data['annotations']\n",
    "            \n",
    "            TEMP_ANNOTATIONS = []\n",
    "            \n",
    "            # Overlay things mask one at a time\n",
    "            for annotation in omask:\n",
    "                # overlay mask of construction things on top of detr output\n",
    "                omask_image_id = overlay_custom_mask.get_overlayed_mask((h, w), annotation)\n",
    "                custom_panoptic_seg_id[omask_image_id.astype(np.bool_)] = custom_panoptic_seg_id.max() + 1\n",
    "                custom_panoptic_segments_info.append({\n",
    "                    'segment_id': custom_panoptic_seg_id.max(), \n",
    "                    'category_id': cat2id[category_name], \n",
    "                    'bbox': annotation['bbox'],\n",
    "                    'area': annotation['area'],\n",
    "                    'iscroud': 0,\n",
    "                    'isthing': 1\n",
    "                })\n",
    "\n",
    "                # append annotation of construction things in json file\n",
    "                annotation[\"category_id\"] = cat2id[category_name]\n",
    "                annotation[\"image_id\"] = image_id\n",
    "                TEMP_ANNOTATIONS.append(annotation)\n",
    "            \n",
    "            # Convert to binary segment\n",
    "            binary_masks = np.zeros((\n",
    "                custom_panoptic_seg_id.max() + 1,\n",
    "                custom_panoptic_seg_id.shape[0],\n",
    "                custom_panoptic_seg_id.shape[1]),\n",
    "                dtype=np.uint8\n",
    "            )\n",
    "                \n",
    "            # for each binary mask, detect contours and create annotation for those contours\n",
    "            if len(unique_category_id):\n",
    "                # Skip the onse which are added by us\n",
    "                for category_id in unique_category_id:\n",
    "                    binary_masks[unique_category_id_to_id[category_id], :, :] = custom_panoptic_seg_id == unique_category_id_to_id[category_id]\n",
    "                    annotation_info = convert_to_coco.main(binary_masks[unique_category_id_to_id[category_id]], None, image_id, category_id, unique_category_id_to_id[category_id], 0)\n",
    "                    if annotation_info is not None:\n",
    "                        annotation_info[\"image_id\"] = image_id\n",
    "                        annotation_info[\"category_id\"] = category_id\n",
    "                        TEMP_ANNOTATIONS.append(annotation_info)\n",
    "                        \n",
    "                        custom_panoptic_segments_info[unique_category_id_to_id[category_id]]['bbox'] = annotation_info['bbox']\n",
    "                        custom_panoptic_segments_info[unique_category_id_to_id[category_id]]['area'] = annotation_info['area']\n",
    "            else:\n",
    "                # Do something for the once where there are no predictions\n",
    "                ## Probably mark them as None\n",
    "                pass\n",
    "            \n",
    "            \n",
    "\n",
    "            # Write data to global json and save files to image dir's\n",
    "            \n",
    "            # save image to new path as .jpg\n",
    "            imo.save(output_file_path)\n",
    "            \n",
    "            # save panoptic image\n",
    "            Image.fromarray(id2rgb(custom_panoptic_seg_id), 'RGB').save(output_mask_path)\n",
    "\n",
    "            # create image_info object and append it to original list\n",
    "            image_info = coco_creator_tools.create_image_info(image_id, output_file_name, imo.size)\n",
    "            image_info[\"original_file\"] = processing_file\n",
    "\n",
    "            GLOBAL_COCO[\"images\"].append(image_info)\n",
    "            GLOBAL_PANOPTIC[\"images\"].append(image_info)\n",
    "\n",
    "            for annotation in TEMP_ANNOTATIONS:\n",
    "                annotation[\"id\"] = annotation_id\n",
    "                GLOBAL_COCO[\"annotations\"].append(annotation)\n",
    "                annotation_id += 1\n",
    "                \n",
    "            for segment_info in custom_panoptic_segments_info:\n",
    "                segment_info[\"id\"] = segment_id\n",
    "                segment_id += 1\n",
    "                \n",
    "            GLOBAL_PANOPTIC[\"annotations\"].append({\n",
    "                \"segments_info\": custom_panoptic_segments_info,\n",
    "                \"file_name\": output_mask_name,\n",
    "                \"image_id\": image_id\n",
    "            })\n",
    "\n",
    "            # increment the image_count\n",
    "            image_id += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            # if there is any error, add info about it in errros file and procees to next image\n",
    "            print(\"Error occurred while processig file:\", processing_file)\n",
    "            \n",
    "            with open(os.path.join(ROOT_DIR, \"error.json\"), 'r') as error_file:\n",
    "                error_json = json.load(error_file)\n",
    "                \n",
    "            with open(os.path.join(ROOT_DIR, \"error.json\"), 'w') as error_file:\n",
    "                error_json[\"error\"].append({\n",
    "                    \"processing_file\": processing_file,\n",
    "                    \"processing_data\": processing_data\n",
    "                })\n",
    "                \n",
    "                json.dump(error_json, error_file)\n",
    "                \n",
    "            traceback.print_exc()\n",
    "    \n",
    "    total_time_str = str(datetime.timedelta(seconds=int(time.time() - start)))\n",
    "    print(f\"Completed Category: {category_name}, Time Taken: {total_time_str}\")\n",
    "\n",
    "    # open the final json, and commit changes in that file\n",
    "    with open(os.path.join(ROOT_DIR, \"coco.json\"), 'w') as output_json_file:\n",
    "        json.dump(GLOBAL_COCO, output_json_file)\n",
    "        \n",
    "    with open(os.path.join(ROOT_DIR, \"panoptic.json\"), 'w') as output_json_file:\n",
    "        json.dump(GLOBAL_PANOPTIC, output_json_file, default=convert)\n",
    "        \n",
    "    print(image_id, annotation_id, segment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects-env",
   "language": "python",
   "name": "projects-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
