{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cu111\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# The GPU id to use, \"0\" to  \"7\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/cocodataset/panopticapi.git\n",
      "  Cloning https://github.com/cocodataset/panopticapi.git to /tmp/pip-req-build-0jw2ui_s\n",
      "  Running command git clone -q https://github.com/cocodataset/panopticapi.git /tmp/pip-req-build-0jw2ui_s\n",
      "  Resolved https://github.com/cocodataset/panopticapi.git to commit 7bb4655548f98f3fedc07bf37e9040a992b054b0\n",
      "Requirement already satisfied: Pillow in /home/ammar/projects/.venv/lib/python3.6/site-packages (from panopticapi==0.1) (8.2.0)\n",
      "Requirement already satisfied: numpy in /home/ammar/projects/.venv/lib/python3.6/site-packages (from panopticapi==0.1) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/cocodataset/panopticapi.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pycocotools in /home/ammar/projects/.venv/lib/python3.6/site-packages (2.0.2)\n",
      "Requirement already satisfied: cython>=0.27.3 in /home/ammar/projects/.venv/lib/python3.6/site-packages (from pycocotools) (0.29.24)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /home/ammar/projects/.venv/lib/python3.6/site-packages (from pycocotools) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=18.0 in /home/ammar/projects/.venv/lib/python3.6/site-packages (from pycocotools) (39.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ammar/projects/.venv/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ammar/projects/.venv/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools) (8.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ammar/projects/.venv/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ammar/projects/.venv/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/ammar/projects/.venv/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools) (1.19.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ammar/projects/.venv/lib/python3.6/site-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n",
      "Requirement already satisfied: six in /home/ammar/projects/.venv/lib/python3.6/site-packages (from cycler>=0.10->matplotlib>=2.1.0->pycocotools) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'detr' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/detr.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), \"detr/\"))\n",
    "# print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device() -> tuple:\n",
    "    \"\"\"Get Device type\n",
    "\n",
    "    Returns:\n",
    "        tuple: Device type\n",
    "    \"\"\"\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    return (use_cuda, device)\n",
    "\n",
    "use_cuda, device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "\n",
    "from categories_meta import COCO_CATEGORIES, COCO_NAMES\n",
    "from panopticapi.utils import id2rgb, rgb2id\n",
    "import panopticapi\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import requests\n",
    "import json\n",
    "import io\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "# Create Original Segmented Image\n",
    "import overlay_custom_mask\n",
    "import convert_to_coco\n",
    "\n",
    "from categories_meta import COCO_CATEGORIES, NEW_CATEGORIES, MAPPINGS, INFO, LICENSES, cat2id, id2cat\n",
    "import coco_creator_tools\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load detr model\n",
    "model, postprocessor = torch.hub.load('detr', 'detr_resnet101_panoptic', source='local', pretrained=True, return_postprocessor=True, num_classes=250)\n",
    "# Convert to eval mode\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "category_paths = glob.glob('/home/ammar/data/construction/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Category: hydra_crane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ammar/projects/.venv/lib/python3.6/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "/home/ammar/projects/.venv/lib/python3.6/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "/home/ammar/projects/.venv/lib/python3.6/site-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
      "/home/ammar/projects/construction/coco_creator_tools.py:48: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  contours = np.subtract(contours, 1)\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = './data'\n",
    "\n",
    "processing_file = \"\"\n",
    "processing_data = []\n",
    "\n",
    "image_id = 1\n",
    "annotation_id = 1\n",
    "\n",
    "GLOBAL_COCO = {\n",
    "    \"licenses\": LICENSES,\n",
    "    \"info\": INFO,\n",
    "    \"categories\": NEW_CATEGORIES,\n",
    "    \"annotations\": [],\n",
    "    \"images\": []\n",
    "}\n",
    "\n",
    "############################ Create DATASET ################################\n",
    "\n",
    "# run through all folders in dataset\n",
    "for category_path in category_paths:\n",
    "    # store starting time\n",
    "    start = time.time()\n",
    "    # get category name\n",
    "    category_name = category_path.split(\"/\")[5]\n",
    "    print(\"Processing Category:\", category_name)\n",
    "    # open category coco file\n",
    "    with open(os.path.join(category_path, \"coco.json\"), \"r\") as coco_file:\n",
    "        category_coco = json.load(coco_file)\n",
    "        \n",
    "    images_root = os.path.join(category_path, 'images')\n",
    "        \n",
    "    # Process all images\n",
    "    ## 1. Create a temp json which contains each image and its annotations\n",
    "    ## 2. Run over this list\n",
    "    ### 1. Copy this image as .jpg in GLOBAL_DIR\n",
    "    ### 2. Find all segments for this image\n",
    "    ### 3. Create new anotation segment which includes annotations from custom classes\n",
    "    \n",
    "    TEMP_COCO_IMAGES = {}\n",
    "    \n",
    "    # Run over all images\n",
    "    for im in category_coco[\"images\"]:\n",
    "        im['annotations'] = []\n",
    "        TEMP_COCO_IMAGES[im['id']] = im\n",
    "        \n",
    "    for ann in category_coco[\"annotations\"]:\n",
    "        TEMP_COCO_IMAGES[ann['image_id']][\"annotations\"].append(ann)\n",
    "        \n",
    "    for i, image_coco in TEMP_COCO_IMAGES.items():\n",
    "        # get image path\n",
    "        ## This data can be used further for logging if failed while processing\n",
    "        processing_file = os.path.join(images_root, image_coco['file_name'])\n",
    "        processing_data = image_coco\n",
    "        output_file_name = category_name + \"_\" + str(image_id) + \".jpg\"\n",
    "        output_file_path = os.path.join(ROOT_DIR, \"images\", output_file_name)\n",
    "        \n",
    "        try:\n",
    "\n",
    "            # Read this image and get shape of image\n",
    "            imo = Image.open(processing_file).convert('RGB')\n",
    "\n",
    "            try:\n",
    "                h, w, c = np.array(imo).shape\n",
    "            except:\n",
    "                h, w = np.array(imo).shape\n",
    "                c = 1\n",
    "\n",
    "            # if no of channels != 3, open the image and convert it to 3 channel - RGB\n",
    "            if c == 4 or c == 1:\n",
    "                imo = imo.convert('RGB')\n",
    "                h, w, c = np.array(imo).shape\n",
    "\n",
    "            # Create a copy of image this will be used for further processing\n",
    "            im = imo.copy()\n",
    "\n",
    "            # Apply transform and convert image to batch\n",
    "            # mean-std normalize the input image (batch-size: 1)\n",
    "            img = transform(im).unsqueeze(0).to(device)  # [h, w, c] -> [1, c, ht, wt]\n",
    "\n",
    "            # Generate output for image\n",
    "            out = model(img)\n",
    "\n",
    "            # Generate score\n",
    "            # compute the scores, excluding the \"no-object\" class (the last one)\n",
    "            scores = out[\"pred_logits\"].softmax(-1)[..., :-1].max(-1)[0]\n",
    "\n",
    "            # threshold the confidence\n",
    "            keep = scores > 0.85\n",
    "\n",
    "            # Keep only ones above threshold\n",
    "            pred_logits, pred_boxes = out[\"pred_logits\"][keep][:, :len(\n",
    "                COCO_NAMES) - 1], out[\"pred_boxes\"][keep]\n",
    "\n",
    "            # the post-processor expects as input the target size of the predictions (which we set here to the image size)\n",
    "            result = postprocessor(out, torch.as_tensor(img.shape[-2:]).unsqueeze(0))[0]\n",
    "\n",
    "            # The segmentation is stored in a special-format png\n",
    "            panoptic_seg = Image.open(io.BytesIO(result['png_string'])).resize((w, h), Image.NEAREST)\n",
    "            # (wp, hp) = panoptic_seg.size\n",
    "            panoptic_seg = np.array(panoptic_seg, dtype=np.uint8).copy()\n",
    "\n",
    "            # We retrieve the ids corresponding to each mask\n",
    "            panoptic_seg_id = rgb2id(panoptic_seg)\n",
    "\n",
    "            # Convert to binary segment\n",
    "            binary_masks = np.zeros((\n",
    "                panoptic_seg_id.max() + 1,\n",
    "                panoptic_seg_id.shape[0],\n",
    "                panoptic_seg_id.shape[1]),\n",
    "                dtype=np.uint8\n",
    "            )\n",
    "\n",
    "            # annotations of our construction things\n",
    "            omask = processing_data['annotations']\n",
    "\n",
    "            # overlay mask of construction things on top of detr output\n",
    "            omask_image_id = overlay_custom_mask.get_overlayed_mask((h, w), omask)\n",
    "\n",
    "            panoptic_seg_id[omask_image_id.astype(np.bool_)] = panoptic_seg_id.max() + 1\n",
    "\n",
    "            TEMP_ANNOTATIONS = []\n",
    "\n",
    "            # append annotation of construction things in json file\n",
    "            for annotation in omask:\n",
    "                annotation[\"category_id\"] = cat2id[category_name]\n",
    "                annotation[\"image_id\"] = image_id\n",
    "                TEMP_ANNOTATIONS.append(annotation)\n",
    "\n",
    "            # for each binary mask, detect contours and create annotation for those contours\n",
    "            if len(result['segments_info']):\n",
    "                for id in np.unique(panoptic_seg_id)[:-1]:  # Skip the last one as it is for custom mappings\n",
    "                    binary_masks[id, :, :] = panoptic_seg_id == id\n",
    "                    annotation_info = convert_to_coco.main(binary_masks[id], None, image_id, result['segments_info'][id][\"category_id\"], result['segments_info'][id][\"id\"], False)\n",
    "                    if annotation_info is not None:\n",
    "                        annotation_info[\"image_id\"] = image_id\n",
    "                        annotation_info[\"category_id\"] = MAPPINGS[annotation_info[\"category_id\"]]\n",
    "                        TEMP_ANNOTATIONS.append(annotation_info)\n",
    "\n",
    "            # Write data to original global json and file to image dir\n",
    "            imo.save(output_file_path)\n",
    "\n",
    "            # create image_info object and append it to original list\n",
    "            image_info = coco_creator_tools.create_image_info(image_id, output_file_name, imo.size)\n",
    "\n",
    "            GLOBAL_COCO[\"images\"].append(image_info)\n",
    "\n",
    "            for annotation in TEMP_ANNOTATIONS:\n",
    "                annotation[\"id\"] = annotation_id\n",
    "                GLOBAL_COCO[\"annotations\"].append(annotation)\n",
    "                annotation_id += 1\n",
    "\n",
    "            # increment the image_count\n",
    "            image_id += 1\n",
    "        \n",
    "        except Exception as e:\n",
    "            # if there is any error, add info about it in errros file and procees to next image\n",
    "            print(\"Error occurred while processig file:\", processing_file)\n",
    "            \n",
    "            with open(os.path.join(ROOT_DIR, \"error.json\"), 'r') as error_file:\n",
    "                error_json = json.load(error_file)\n",
    "                \n",
    "            with open(os.path.join(ROOT_DIR, \"error.json\"), 'w') as error_file:\n",
    "                error_json[\"error\"].append({\n",
    "                    \"processing_file\": processing_file,\n",
    "                    \"processing_data\": processing_data\n",
    "                })\n",
    "                \n",
    "                json.dump(error_json, error_file)\n",
    "            print(e)\n",
    "                \n",
    "    print(f\"Completed Category: {category_name}, Time Taken: {(time.time() - start)/60} minutes\")\n",
    "\n",
    "    # open the final json, and commit changes in that file\n",
    "    with open(os.path.join(ROOT_DIR, \"coco.json\"), 'w') as output_json_file:\n",
    "        json.dump(GLOBAL_COCO, output_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects-env",
   "language": "python",
   "name": "projects-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
